Нейросети для детей: объясняем максимально просто

Всем привет! Сегодня я хочу рассказать о том, как устроены нейросети, но так просто, что понять могут даже начинающие, которые освоили только самые основы питона.

<h3> Что такое вообще нейронные сети и искусственный интеллект? </h3>
В философии ИИ существует два вида искусственного интеллекта - сильный и слабый.  Теория сильного искусственного интеллекта предполагает, что компьютеры смогут обрести способность мыслить и осознавать себя как отдельную личность (ну примерно как в Detroit Become Human). Теория слабого искусственного интеллекта предполагает, что такой интеллект нельзя сконструировать, и пока что наука особо способна создавать нейросети, которые только частично повторяют работу нейросетей живых существ. Но философия как раз и не относится к науке, потому что там ничего нельзя доказать, поэтому заострять наше внимания на этом не будем. Но слабые нейронные сети сейчас занимают доминирующее положение в Data Science, и применяются широко: в распознавании визуальных образов, умной закупке товаров, даже системе ОБС автомобиля – поэтому начать учить ИИ сейчас вполне актуально.
<h3>А что такой нейронные сети </h3>
Нейронные сети представляют собой математические модели работы реальных нейронных сетей живых существ. Математическую модель достаточно просто перевести в программу (и из-за этого в программировании нейронных сетей так широко используется Python, столь удобный для программирования решения математических задач. На самом деле нейронные сети можно написать на практически любом языке программирования, в котором поддерживается какая-никакая математика. Даже на Scratch, который изначально создан для обучения основам программирования младших школьников. Можно посмотреть <a href="https://scratch.mit.edu/projects/26859854/"> здесь </a> ).
<h3> Основные понятия нейросетей </h3>
Существует множество алгоритмов работы нейросетей (и сейчас математическая сторона этого вопроса активно разрабатывается). Классическим решением для новичков является метод обратного распространения ошибок(backpropagation)  - метод вычисления градиента, который используется для обновления весов многослойного перцептрона. В том виде, в котором его обычно обычно изучают новички( с сигмооидной функцией активации) нейросеть получается достаточно медленная, но относительно точная.
Стоп? Что такое вообще веса?
Та программа, которую мы собираемся написать называется нейронной сетью с огромной натяжкой.
Прежде чем перейти к ее описанию, давайте обсудим, чем вообще занимаются нейронные сети.

<h3> Чем вообще занимаются нейросети, если все упростить </h3>
Если немного упростить концепцию нейронных сетей, то нейросети, которые обучаются по принципу обучения с учителем, после обучения по принципу "стимул - рекция", с указанием правильных ответов, она может работать с <i> незнакомыми </i> данными. Иными словами, если вы предложили нейросети на вход некий набор слов (например, набор отзывов на кинопоиске, положительных и отрицательных, в любом формате, хоть txt, хоть json, здесь вопрос стоит только в программе обработки этих данных). Для успешного создания нейросети вам потребуются два набора данных: тестовой набор, с помощью которого можно будет оценить эффективность работы созданной нейросети, и обучающий набор, в котором для нее данные размечены на положительные/отрицательные (и здесь возникает проблема классификации больших данных, потому что это долгое и муторное занятие). После обучения нейросети (которое может занять много времени времени и ресурсов компьютера, в зависимости от размерности данных, их обработки, и что чаще всего самое главное, применяемых алгоритмов), она сможет <i> пытаться</i> предугададывать с некоторой точностью, положительный или отрицательный отзыв к ней пришел на вход. Но нейросети (как впрочем и реальный человек), имеют некоторый процент ошибки. Задача оптимизации - сделать его минимальным, но вопрос оценки качества нейросетей скорее всего никогда никуда не денется. К примеру, вам дают фотографию хот-дога, и здесь вы точно говорите, что это хот-дог. Но что, если фотография будет смазана? Черно-белая? Снята в плохой видимости? Здесь вы сможете уже утверждать только с некоторой долей вероятности, даже если приготовили или съели немало хот-догов в своей жизни.

<h3> Начинаем програмировать нашу игрушечную нейросеть </h3>
Окей, поехали. В нашей игрушечной нейросети не будет тестовых и тренировочных данных, наша игрушечная нейросеть будет пытаться найти <i> коэффициент соотношения</i> между любыми данными.
Стоп. А какой в этом смысл? Это же находится одним простым математическим выражением.
Вне сомнения. Однако сейчас я взял это выражение, чтобы показать процесс обучения нейросети. Допустим, перед нами стоит задача выяснить, какой коэффициент именно перевода между дюймами и сантиметрами. Сколько дюйма занимает один сантиметр? Для человека, который знает математику хотя бы пятого класса (а то и раньше), не составляет труда вспомнить, какой коэффициент перевода – 2.54.
Но сейчас мы на время забудем об этом, и представим, что нам нужно создать простой алгоритм, который будет универсально вычислять этот параметр. Однако загвоздка ещё и в том, что нейросети не являются некоторыми константами значений с готовыми значениями коэффициентов, иначе бы в них не было "живого" обучения. 
Итого, мы в положении ребенка, который только сел перед набором кубиков, и собирается взять их впервые в свои руки и соорудить первую в своей жизни башенку. Он только примерно знает, как работает физика предметов, он также как и мы знает, что какой-то определенный коэффициент соотношения существует ( в его случае это гравитация). Так что сделает ребенок? Он возьмет и наугад поставит какой-то кубик. Точно также мы можем только наугад предположить, какой у нас будет коэффициент (и реальные взрослые нейросети тоже так делают, только обычно руководствуясь некоторой генерацией в нормальном распределении). Просто наугад предположим, что коэффициент связи дюймов и сантиметров (давайте начнем его называть <b>вес</b>, как во взрослых нейросетях) будет равен, к примеру, 2.4. Тогда у нас получится сложнейшее математическое выражение:
<source>
1 * 2.4 = 2.4
</source>
Здорово, мы почти угадали, и у нас есть некоторый результат.  Но он неверный, и что нормально для процесса обучения, у нас есть некоторая ошибка. Как нервная система с обратной связью, нам нужно как-то среагировать на ошибку. Но сначала нужно понять ее размер. Как я уже говорил, в нейросетях обучения с учителем сначала данные прогоняют на размеченных данных, и только потом уже отправляют в классификацию на похожих, но неразмеченных. Так же и мы знаем, какой у нас должен получится правильный результат, и я смогу подсчитать ошибку:
<source>
t_target = 2.54
t_now = 2.40
e = t_target - t_now
//получаем значение е, равное сейчас 0.14
</source>   
Теперь мы знаем, насколько ошиблись. Но что нам делать? Естественно, наша игрушеная нейросеть должна считать данные как можно с наименьшим коэффициентом ошибки. И в этом проявляется ещё одна подводная особенность нейросетей - чаще всего они имеют некоторый коэффициент потери при обучении, минимизацией ошибки занимается часть Data Science, под названием <a href="https://ru.wikipedia.org/wiki/%D0%9E%D0%BF%D1%82%D0%B8%D0%BC%D0%B8%D0%B7%D0%B0%D1%86%D0%B8%D1%8F_(%D0%BC%D0%B0%D1%82%D0%B5%D0%BC%D0%B0%D1%82%D0%B8%D0%BA%D0%B0)">оптимизация</a>. 
Но сейчас не об этом. Вне сомнения, нам нужно начать изменять наш коэффициент связи на какой-то шаг, но на какой? Очевидно, что надо не слишком большой, а то наш коэффициент связи получится неточным, и не слишком маленький, иначе нам придется довольно долго обучать нашу нейросеть. Здесь особо нет правильного решения, чаще всего эти параметры и в реальной нейросети подбираются скорее интуитивно, чем на основании какой-то формулы или алгоритма (хотя такое тоже возможно).  Сейчас мы можем наугад выбрать значение нашего шага (на языке нейростей наш шаг называется learning_rate), к примеру, достаточно оптимально будет работать значение 0.05. Кроме этого, нужно договориться о том, сколько раз у нас будет происходить отступ на <b>learning rate</b>. Количество этих отступов мы назовем эпохи, как во взрослых нейросетях. Теперь, вооружившись всеми этими знаниями, мы можем попробовать написать небольшую программу на Python, которая будет выполнять нашу программу "игрушечной" нейросети.
<source lang="python"> 
import random

# возьмем встроенную в Python библиотеку random, что мы могли задавать случайный вес нашей нейросети

inches = 40  # мы знаем что 40 дюймов равняется примерно 101, 6 сантиметрам
centimetre = 101.6

# подбор коэффициента связи. Уже по-взрослому называю его весом

# создаем функцию, которая занимается подбором и принимает на вход эпохи, learning rate и точность

def kid_neuro(epoch, lr, accur):
    W_coef = random.uniform(0, 2)  # получаем наш случайный вес связи
    print("Наш первоначальный случайный вес равен: ", W_coef)
    for i in range(epoch):  # воспользуемся циклом для прокрутки
        Error = centimetre - (inches * W_coef)
        print("Наша ошибка составляет", Error)  # будем печатать ошибку для нашей визуализации
        if Error > 0:
            W_coef += lr  # если ошибка слишком большая, мы начинаем прибавлять коэффициент

        if Error < 0:
            W_coef -= lr  # если ошибка отрицательная, тогда начинаем уменьшать коэффициент

        if Error < accur:
            print("Наш итоговый результат", W_coef)
            return  # эффектно вычисляем, сколько же сантиметров в одном дюйме

epoch = int(input("epoch: "))  # эпохи это у нас количество "прогонов"
lr = float(input("enter learning rate: "))  # наш шаг обучения
accur = float(input(
    "enter accurancy:  "))  # нам нужно уточнить, на какую точность мы согласны, потому что идеальной у нас скорее всего не получится

kid_neuro(epoch, lr, accur)
</source>
Я оставляю читателя самостоятельно попробовать позапускать эту детскую нейронную сеть с различными параметрами. Неплохо получается на epoch = 100-, learning rate = 0.01, accur = 0.1.
Несмотря на кажущуюся бесполезность этой программы, мы разобрали с вами работу и основные понятия нейронных сетей, которые используются и в построении реальных больших нейронных сетей, к примеру в алгоритме обратного распространения ошибки(backprogation). Вкратце, эти основные понятия:
<ul>
    <li> <b>W</b> - вес. Вес обычно показывает связь узла нейронной сети с каким-то понятием, если нейросеть настроена на классификацию. В нашей программе был только один вес - коэффициент взаимосвязи между дюймами и сантиметрами, но обновляли мы его примерно также, как в реальной неройсети </li>
    <li>
        <b>lr</b> - learning rate, или скорость обучения. Показывает с каким шагом мы будем <i>обновлять</i> нашу связь при каждом прогоне
    </li>
    <li> <b>epoch</b> Эпохи, или сколько прогонов у нас будет для достижения максимально точного результата </li>
</ul>

В качестве практики вы можете самостоятельно попробовать написать свою детскую нейросеть, которая будет переводить, к примеру, километры в мили. И вооружившись полученными в данной статье знаниями, вы сможете без проблем прийти например <a href="https://playground.tensorflow.org/">сюда</a> , и уже попробовать позапускать нейросеть более осмысленно.

Несколько полезных ссылок, на которые вы можете перейти, если желаете продолжить обучение нейросетям:
<a href="https://habr.com/ru/post/312450/"> Хорошая серия статей про нейросети на русском на Хабре </a>
<a href="https://www.allaboutcircuits.com/technical-articles/how-to-perform-classification-using-a-neural-network-a-simple-perceptron-example/"> Перцептроны в одиночку используются в современных нейросетях не очень широко. Но начать свой путь именно с них хорошая идея</a>
<a href="https://medium.com/@mjbhobe/mnist-digits-classification-with-keras-ed6c2374bd0e"> Если вы собираетесь развиваться в нейросетях, написать что-то на Mnist – что-то навроде Hello World </a>















































